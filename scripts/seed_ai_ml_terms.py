"""
Seed AI/ML Terms Script - Populate knowledge_base with 20 advanced AI/ML terms.

Tags: Class: Tech, Difficulty: Advanced
"""

from __future__ import annotations

import sys
from datetime import datetime, timezone
from pathlib import Path

# Fix Windows console encoding
if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        pass

# Add project root to path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from sqlalchemy.orm import Session

from database.db_manager import KnowledgeBase, SessionLocal, init_db

# 20 Advanced AI/ML Terms with Japanese technical loanwords
AI_ML_TERMS = [
    {
        "concept_title": "Transformer Architecture",
        "japanese_term": "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£",
        "concept_content": """Transformer Architecture (ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)

A deep learning model architecture introduced in "Attention Is All You Need" (2017). 
Uses self-attention mechanisms to process sequences in parallel rather than sequentially.

Key Components:
- Self-Attention (ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³): Allows the model to weigh the importance of different parts of the input
- Multi-Head Attention (ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³): Multiple attention mechanisms run in parallel
- Positional Encoding (ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°): Adds information about word order
- Feed-Forward Networks (ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯): Processes attended features

Applications: GPT, BERT, modern language models

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Gradient Descent",
        "japanese_term": "å‹¾é…é™ä¸‹æ³•",
        "concept_content": """Gradient Descent (å‹¾é…é™ä¸‹æ³• - ã“ã†ã°ã„ã“ã†ã‹ã»ã†)

An optimization algorithm used to minimize a loss function by iteratively moving in the direction of steepest descent.

Types:
- Batch Gradient Descent (ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•): Uses entire dataset
- Stochastic Gradient Descent (ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•): Uses one sample at a time
- Mini-Batch Gradient Descent (ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•): Uses small batches

Learning Rate (å­¦ç¿’ç‡): Controls step size in parameter space.

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Latent Space",
        "japanese_term": "æ½œåœ¨ç©ºé–“",
        "concept_content": """Latent Space (æ½œåœ¨ç©ºé–“ - ã›ã‚“ã–ã„ãã†ã‹ã‚“)

A compressed representation space where similar data points are close together. 
Used in autoencoders, GANs, and variational models.

Properties:
- Dimensionality Reduction (æ¬¡å…ƒå‰Šæ¸›): Maps high-dimensional data to lower dimensions
- Feature Learning (ç‰¹å¾´å­¦ç¿’): Learns meaningful representations
- Interpolation (è£œé–“): Smooth transitions between data points

Applications: Image generation, style transfer, anomaly detection

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Neural Network",
        "japanese_term": "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "concept_content": """Neural Network (ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)

A computing system inspired by biological neural networks. Consists of interconnected nodes (neurons) organized in layers.

Architecture:
- Input Layer (å…¥åŠ›å±¤): Receives data
- Hidden Layers (éš ã‚Œå±¤): Process information
- Output Layer (å‡ºåŠ›å±¤): Produces predictions

Activation Functions (æ´»æ€§åŒ–é–¢æ•°): Introduce non-linearity (ReLU, Sigmoid, Tanh)

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Backpropagation",
        "japanese_term": "èª¤å·®é€†ä¼æ’­æ³•",
        "concept_content": """Backpropagation (èª¤å·®é€†ä¼æ’­æ³• - ã”ã•ãã‚ƒãã§ã‚“ã±ã»ã†)

Algorithm for training neural networks by propagating errors backward through the network.

Process:
1. Forward Pass (é †ä¼æ’­): Compute predictions
2. Calculate Loss (æå¤±è¨ˆç®—): Compare predictions to targets
3. Backward Pass (é€†ä¼æ’­): Compute gradients
4. Update Weights (é‡ã¿æ›´æ–°): Adjust parameters using gradients

Chain Rule (é€£é–å¾‹): Mathematical foundation for computing gradients

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Convolutional Neural Network",
        "japanese_term": "ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "concept_content": """Convolutional Neural Network (ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ - ãŸãŸã¿ã“ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)

Deep learning architecture designed for processing grid-like data (images, time series).

Key Components:
- Convolutional Layers (ç•³ã¿è¾¼ã¿å±¤): Apply filters to detect features
- Pooling Layers (ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤): Reduce spatial dimensions
- Fully Connected Layers (å…¨çµåˆå±¤): Final classification/regression

Applications: Image recognition, computer vision, medical imaging

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Recurrent Neural Network",
        "japanese_term": "å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "concept_content": """Recurrent Neural Network (å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ - ã•ã„ããŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)

Neural network architecture with feedback connections, designed for sequential data.

Variants:
- LSTM (Long Short-Term Memory): é•·çŸ­æœŸè¨˜æ†¶ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- GRU (Gated Recurrent Unit): ã‚²ãƒ¼ãƒˆä»˜ãå›å¸°ãƒ¦ãƒ‹ãƒƒãƒˆ
- Bidirectional RNN (åŒæ–¹å‘RNN): Processes sequences in both directions

Applications: Language modeling, speech recognition, time series prediction

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Generative Adversarial Network",
        "japanese_term": "æ•µå¯¾çš„ç”Ÿæˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "concept_content": """Generative Adversarial Network (æ•µå¯¾çš„ç”Ÿæˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ - ã¦ããŸã„ã¦ãã›ã„ã›ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)

Two neural networks competing: Generator (ç”Ÿæˆå™¨) creates fake data, Discriminator (è­˜åˆ¥å™¨) distinguishes real from fake.

Training Process:
- Generator learns to fool discriminator
- Discriminator learns to detect fakes
- Adversarial training improves both

Applications: Image generation, style transfer, data augmentation

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Attention Mechanism",
        "japanese_term": "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹",
        "concept_content": """Attention Mechanism (ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹)

Allows models to focus on relevant parts of input when making predictions.

Types:
- Self-Attention (ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³): Attention within same sequence
- Cross-Attention (ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³): Attention between different sequences
- Multi-Head Attention (ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³): Multiple attention heads in parallel

Key Innovation: Enables parallel processing and long-range dependencies

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Reinforcement Learning",
        "japanese_term": "å¼·åŒ–å­¦ç¿’",
        "concept_content": """Reinforcement Learning (å¼·åŒ–å­¦ç¿’ - ãã‚‡ã†ã‹ãŒãã—ã‚…ã†)

Machine learning paradigm where agents learn by interacting with environment through rewards and penalties.

Components:
- Agent (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ): Learning entity
- Environment (ç’°å¢ƒ): External system
- Reward Signal (å ±é…¬ä¿¡å·): Feedback mechanism
- Policy (æ–¹ç­–): Strategy for action selection

Applications: Game playing, robotics, autonomous systems

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Transfer Learning",
        "japanese_term": "è»¢ç§»å­¦ç¿’",
        "concept_content": """Transfer Learning (è»¢ç§»å­¦ç¿’ - ã¦ã‚“ã„ãŒãã—ã‚…ã†)

Technique of reusing a pre-trained model on a new, related task.

Process:
1. Pre-train on large dataset (äº‹å‰å­¦ç¿’)
2. Fine-tune on target task (ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
3. Transfer knowledge (çŸ¥è­˜è»¢ç§»)

Benefits: Faster training, better performance with less data

Applications: Computer vision, NLP, domain adaptation

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Hyperparameter Tuning",
        "japanese_term": "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´",
        "concept_content": """Hyperparameter Tuning (ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´)

Process of finding optimal hyperparameters (parameters set before training).

Methods:
- Grid Search (ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ): Exhaustive search over parameter grid
- Random Search (ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ): Random sampling of parameters
- Bayesian Optimization (ãƒ™ã‚¤ã‚ºæœ€é©åŒ–): Probabilistic model-based optimization

Common Hyperparameters: Learning rate, batch size, network depth

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Overfitting",
        "japanese_term": "éå­¦ç¿’",
        "concept_content": """Overfitting (éå­¦ç¿’ - ã‹ãŒãã—ã‚…ã†)

When a model learns training data too well, including noise, and fails to generalize to new data.

Symptoms:
- High training accuracy, low validation accuracy
- Large gap between train and validation loss

Solutions:
- Regularization (æ­£å‰‡åŒ–): L1/L2 penalties
- Dropout (ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ): Randomly disable neurons
- Early Stopping (æ—©æœŸåœæ­¢): Stop training when validation loss increases
- Data Augmentation (ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ): Increase dataset diversity

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Batch Normalization",
        "japanese_term": "ãƒãƒƒãƒæ­£è¦åŒ–",
        "concept_content": """Batch Normalization (ãƒãƒƒãƒæ­£è¦åŒ–)

Technique to normalize inputs of each layer by adjusting and scaling activations.

Benefits:
- Faster training convergence
- Allows higher learning rates
- Reduces internal covariate shift
- Acts as regularization

Process: Normalize â†’ Scale â†’ Shift using learnable parameters

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Word Embedding",
        "japanese_term": "å˜èªåŸ‹ã‚è¾¼ã¿",
        "concept_content": """Word Embedding (å˜èªåŸ‹ã‚è¾¼ã¿ - ãŸã‚“ã”ã†ã‚ã“ã¿)

Dense vector representations of words that capture semantic relationships.

Methods:
- Word2Vec (ãƒ¯ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ™ãƒƒã‚¯): Predicts context or target words
- GloVe (ã‚°ãƒ­ãƒ¼ãƒ–): Global vectors from word co-occurrence
- FastText (ãƒ•ã‚¡ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ): Character-level n-grams

Properties: Similar words have similar vectors, arithmetic operations capture relationships

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Feature Engineering",
        "japanese_term": "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°",
        "concept_content": """Feature Engineering (ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°)

Process of selecting, modifying, or creating features to improve model performance.

Techniques:
- Feature Selection (ç‰¹å¾´é¸æŠ): Choose relevant features
- Feature Transformation (ç‰¹å¾´å¤‰æ›): Normalize, scale, encode
- Feature Creation (ç‰¹å¾´ä½œæˆ): Combine or derive new features
- Dimensionality Reduction (æ¬¡å…ƒå‰Šæ¸›): PCA, t-SNE

Importance: Often more impactful than algorithm choice

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Ensemble Learning",
        "japanese_term": "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’",
        "concept_content": """Ensemble Learning (ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’)

Combining multiple models to improve predictions beyond individual models.

Methods:
- Bagging (ãƒã‚®ãƒ³ã‚°): Bootstrap aggregating (Random Forest)
- Boosting (ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°): Sequential model training (AdaBoost, XGBoost)
- Stacking (ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°): Meta-learner combines base models
- Voting (æŠ•ç¥¨): Majority or weighted voting

Principle: Wisdom of the crowd - multiple weak learners â†’ strong learner

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Cross-Validation",
        "japanese_term": "äº¤å·®æ¤œè¨¼",
        "concept_content": """Cross-Validation (äº¤å·®æ¤œè¨¼ - ã“ã†ã•ã‘ã‚“ã—ã‚‡ã†)

Resampling technique to assess model performance and prevent overfitting.

Types:
- K-Fold (Kåˆ†å‰²): Split data into k folds, train on k-1, test on 1
- Stratified K-Fold (å±¤åŒ–Kåˆ†å‰²): Maintains class distribution
- Leave-One-Out (ä¸€æŠœãäº¤å·®æ¤œè¨¼): Each sample as test set once
- Time Series CV (æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼): Respects temporal order

Purpose: Better estimate of model generalization

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Activation Function",
        "japanese_term": "æ´»æ€§åŒ–é–¢æ•°",
        "concept_content": """Activation Function (æ´»æ€§åŒ–é–¢æ•° - ã‹ã£ã›ã„ã‹ã‹ã‚“ã™ã†)

Non-linear function applied to neuron outputs to introduce non-linearity into neural networks.

Common Functions:
- ReLU (Rectified Linear Unit): æ­£è¦åŒ–ç·šå½¢ãƒ¦ãƒ‹ãƒƒãƒˆ - f(x) = max(0, x)
- Sigmoid (ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰): S-shaped curve, outputs 0-1
- Tanh (ã‚¿ãƒ³ã‚¸ã‚§ãƒ³ãƒˆåŒæ›²ç·š): Outputs -1 to 1
- Softmax (ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹): Multi-class probability distribution

Purpose: Enables networks to learn complex patterns

Difficulty: Advanced
Class: Tech""",
    },
    {
        "concept_title": "Loss Function",
        "japanese_term": "æå¤±é–¢æ•°",
        "concept_content": """Loss Function (æå¤±é–¢æ•° - ãã‚“ã—ã¤ã‹ã‚“ã™ã†)

Function that measures the difference between predicted and actual values.

Types:
- Mean Squared Error (å¹³å‡äºŒä¹—èª¤å·®): For regression
- Cross-Entropy Loss (äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±): For classification
- Binary Cross-Entropy (äºŒå€¤äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼): For binary classification
- Hinge Loss (ãƒ’ãƒ³ã‚¸æå¤±): For SVM

Purpose: Guides optimization during training

Difficulty: Advanced
Class: Tech""",
    },
]


def seed_ai_ml_terms():
    """Seed the knowledge_base with 20 advanced AI/ML terms."""
    print("=" * 60)
    print("ğŸ¤– Seeding Advanced AI/ML Terms")
    print("=" * 60)
    
    # Initialize database
    init_db()
    
    db: Session = SessionLocal()
    try:
        seeded_count = 0
        updated_count = 0
        
        for term_data in AI_ML_TERMS:
            # Check if entry already exists
            existing = db.query(KnowledgeBase).filter(
                KnowledgeBase.concept_title == term_data["concept_title"],
                KnowledgeBase.category == "tech"
            ).first()
            
            if existing:
                # Update existing entry
                existing.concept_content = term_data["concept_content"]
                existing.updated_at = datetime.now(timezone.utc)
                updated_count += 1
                print(f"âœ… Updated: {term_data['concept_title']} ({term_data['japanese_term']})")
            else:
                # Create new entry
                kb_entry = KnowledgeBase(
                    source_file="ai_ml_advanced_terms",
                    concept_title=term_data["concept_title"],
                    concept_content=term_data["concept_content"],
                    language="en",  # English with Japanese technical terms
                    category="tech",
                    page_number=None,
                )
                db.add(kb_entry)
                seeded_count += 1
                print(f"âœ… Added: {term_data['concept_title']} ({term_data['japanese_term']})")
        
        db.commit()
        
        print("\n" + "=" * 60)
        print(f"ğŸ‰ Seeding Complete!")
        print(f"   Added: {seeded_count} entries")
        print(f"   Updated: {updated_count} entries")
        print(f"   Total: {len(AI_ML_TERMS)} AI/ML terms")
        print("=" * 60)
        
    except Exception as e:
        db.rollback()
        print(f"âŒ Error seeding AI/ML terms: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


if __name__ == "__main__":
    seed_ai_ml_terms()

